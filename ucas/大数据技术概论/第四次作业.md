## 大数据技术概论-第四次作业（第五课）

学号：2019Z8020661027

姓名：乔咏田

## 题目

- 输入N个文件，生成带详细信息的倒排索引。

- 举例如下，有4个输入文件：

  - d1.txt：cat dog cat fox
  - d2.txt：cat bear cat cat fox
  - d3.txt：fox wolf dog
  - d4.txt：wolf hen rabbit cat sheep

- 要求建立如下格式的倒排索引：
  - cat—> 3：{(d1.txt, 2, 4),(d2.txt,3,5),(d4.txt,1,5)}
  - 单词—>出现该单词的文件个数：{(出现该单词的文件名，单词在该文件中的出现次数，该文件的总单词数)，……}

## 具体要求

- 1、提供程序设计的伪代码

  - 要求结构清晰，可读性强，并充分考虑程序的性能及可扩展性
- 2、详细说明程序的整体设计思路，逻辑结构，核心模块的设计思想、逻辑结构、输入输出格式及内部处理计算过程
- 3、编程实现该程序设计，提供程序运行环境说明、程序的源代码及运行结果，要求代码有必要的注释和相关说明。

## 伪代码

~~~
public static void main(){
	1、设置job
	2、设置mapper class，combiner class，reducer class
	3、设置output的key与value类型
	4、设置输入FileInputFormat
	5、设置输出FileOutputFormat，若输出目录已存在，则删除
	6、设置job完成和退出状态
}
public static class TokenizerMapper{
	public void map(){
		1、提取fileName
		2、file内容分词
		3、file内单词统计总数
		4、对每个单词进行输出，<word@@@fileName@@@fileWordsSize, 1>
	}
}
public static class ReverseIndexCombiner{
	public void reduce(){
		1、获取mapper的输入<word@@@fileName@@@fileWordsSize, [1, 1, 1, ……]>
		2、统计词频freq
		3、输出：<word, freq@@@fileName@@@fileWordsSize>
	}
}
public static class ReverseIndexReducer{
	public void reduce(){
		1、获取combiner的输出<word, [freq@@@fileName@@@fileWordsSize, freq@@@fileName@@@fileWordsSize, ……]>
		2、组装成最终格式。<cat—> 3：{(d1.txt, 2, 4),(d2.txt,3,5),(d4.txt,1,5)}, "">
	}
}
~~~

## 整体设计思路

1. MapReduce读取数据，使用FileInputFormat读取文件内的数据。
2. Mapper：map函数对文件内容进行分词，并对文件内的单词进行计数，输出为`<word@@@fileName@@@fileWordsSize, 1>`。
3. Combiner：reduce函数输入为`<word@@@fileName@@@fileWordsSize, [1, 1, 1, ……]>`，对单词词频进行汇总，输出为`<word, freq@@@fileName@@@fileWordsSize>`。
4. Reducer：reduce函数输入为`<word, [freq@@@fileName@@@fileWordsSize, freq@@@fileName@@@fileWordsSize, ……]>`，汇总为最终结果，输出为`<word —> count：{(fileName, freq, fileWordsSize),(d2.txt,3,5),(d4.txt,1,5)}, "">`。

## 程序设计实现

- 解压文件

~~~
tar -xzvf Shakespeare.tar.gz
~~~

- 示例文件：运行Jar包

~~~
./bin/hadoop jar ./share/myjar/ucas1211.jar course.ReverseIndex ./input/reverse_index_example/*.txt ./output/reverse_index_1210/result_example
~~~

- 示例文件运行结果：

![image-20191212230955563](%E7%AC%AC%E5%9B%9B%E6%AC%A1%E4%BD%9C%E4%B8%9A.assets/image-20191212230955563.png)

![image-20191212231103273](%E7%AC%AC%E5%9B%9B%E6%AC%A1%E4%BD%9C%E4%B8%9A.assets/image-20191212231103273.png)

- 莎士比亚文件：运行jar包

~~~
./bin/hadoop jar ./share/myjar/ucas1211.jar course.ReverseIndex ./input/reverse_index_1210/*.txt ./output/reverse_index_1210/result
~~~

- 莎士比亚运行结果：

![image-20191212231306977](%E7%AC%AC%E5%9B%9B%E6%AC%A1%E4%BD%9C%E4%B8%9A.assets/image-20191212231306977.png)

![image-20191212231348675](%E7%AC%AC%E5%9B%9B%E6%AC%A1%E4%BD%9C%E4%B8%9A.assets/image-20191212231348675.png)

- 运行环境为：Hadoop-3.2.1，系统为centos7
- 源代码

~~~java
package course;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class ReverseIndex {

	public static class TokenizerMapper extends Mapper<Object, Text, Text, Text> {

		private Text keyInfo = new Text();
		private Text valueInfo = new Text("1");

		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
			// input fileContext and fileName
			// output <word@@@fileName@@@fileWordsSize, 1>
			// get fileName
			FileSplit fileSplit = (FileSplit) context.getInputSplit();
			String fileName = fileSplit.getPath().getName();
			// word segment
			StringTokenizer stringTokenizer = new StringTokenizer(value.toString());
			// word count
			List<String> words = new ArrayList<String>();
			while (stringTokenizer.hasMoreTokens()) {
				String word = stringTokenizer.nextToken();
				// only write not none word
				word = word.trim();
				if (word.length() > 1) {
					words.add(word);
				}
			}
			int fileWordsSize = words.size();
			System.out.println("fileName:" + fileName);
			System.out.println("fileWordsSize:" + fileWordsSize);
			for(String word : words) {
				keyInfo.set(word + "@@@" + fileName + "@@@" + fileWordsSize);
				context.write(keyInfo, valueInfo);
			}
		}
	}
	
	public static class ReverseIndexCombiner extends Reducer<Text, Text, Text, Text>{
		
		private Text keyInfo = new Text();
		private Text valueInfo = new Text();

		public void reduce(Text key, Iterable<Text> values, Context context)
				throws IOException, InterruptedException {
			// input <word@@@fileName@@@fileWordsSize, [1, 1, 1, ……]>
			// output <word, freq@@@fileName@@@fileWordsSize>
			int freq = 0;
			for (Text val : values) {
				freq += 1;
			}
			String[] keyTokens = key.toString().split("@@@");
			keyInfo.set(keyTokens[0]);
			String fileName = keyTokens[1];
			String fileWordsSize = keyTokens[2];
			valueInfo.set(freq + "@@@" + fileName + "@@@" + fileWordsSize);
			context.write(keyInfo, valueInfo);
		}
	}

	public static class ReverseIndexReducer extends Reducer<Text, Text, Text, Text> {

		private Text keyInfo = new Text();
		private Text valueInfo = new Text("");

		public void reduce(Text key, Iterable<Text> values, Context context)
				throws IOException, InterruptedException {
			// input <word, [freq@@@fileName@@@fileWordsSize, freq@@@fileName@@@fileWordsSize, ……]>
			// output <cat—> 3：{(d1.txt, 2, 4),(d2.txt,3,5),(d4.txt,1,5)}, "">
			StringBuilder sb = new StringBuilder();
			sb.append(key);
			sb.append(" -> ");
			int freq = 0;
			StringBuilder subSb = new StringBuilder();
			for (Text val : values) {
				freq += 1;
				String[] keyTokens = val.toString().split("@@@");
				subSb.append("(");
				subSb.append(keyTokens[1]); //add fileName
				subSb.append(", ");
				subSb.append(keyTokens[0]); //add freq
				subSb.append(", ");
				subSb.append(keyTokens[2]); //add fileWordsSize
				subSb.append("),");
			}
			sb.append(freq);
			sb.append(" : {");
			sb.append(subSb.toString());
			sb.append("}");
			keyInfo.set(sb.toString());
			System.out.println(keyInfo.toString());
			context.write(keyInfo, valueInfo);
		}
	}


	public static void main(String[] args) throws Exception {
		System.out.println("main args:");
		for (int i = 0; i < args.length; i++) {
			System.out.println("args " + i + " : " + args[i]);
		}
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf, "ReverseIndex");
		job.setJarByClass(ReverseIndex.class);
		// set Mapper class, Combiner class , Reducer class
		job.setMapperClass(TokenizerMapper.class);
		job.setCombinerClass(ReverseIndexCombiner.class);
		job.setReducerClass(ReverseIndexReducer.class);
		// set output key and value class
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		for (int i = 0; i < args.length - 1; ++i) {
			System.out.println("Input file " + i + " : " + args[i]);
			FileInputFormat.addInputPath(job, new Path(args[i]));
		}
		System.out.println("OutputPath : " + args[args.length - 1]);
		Path outputPath = new Path(args[args.length - 1]);
		FileSystem fileSystem = FileSystem.get(conf);
		if (fileSystem.exists(outputPath)) {
			fileSystem.delete(outputPath, true);
        }
		FileOutputFormat.setOutputPath(job, outputPath);
		boolean result = job.waitForCompletion(true);
		if (result) {
            System.out.println("job is finished");
        }
		System.exit(result ? 0 : 1);
	}
}
~~~

