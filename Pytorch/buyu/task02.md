## 任务名称 

文本预处理；语言模型；循环神经网络基础

## 学习心得

1. 鉴于自己从事NLP已有一段时间，因此该部分还是稍微轻松一些。

2. 以前总是直接用开源的已经训练好的语言模型，大部分都是word2vec的预训练模型，以及后面新出现的比较强大的BERT等。没有自己动手去研究语言模型的来龙去脉，正好借此机会补习一下。

3. 两种采样方式：

- **随机采样：**在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。
- **相邻采样：**在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。

4. 语言模型困惑度（perplexity）：用来评价语言模型的好坏。困惑度是对交叉熵损失函数做指数运算后得到的值。

- 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；
- 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；
- 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。
- 显然，任何一个***有效模型***的困惑度必须小于类别个数。
- 困惑度（perplexity）的基本思想是：**给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，**公式如下：



![img](https://upload-images.jianshu.io/upload_images/8518346-13de37afdd658b68.png?imageMogr2/auto-orient/strip|imageView2/2/w/456/format/webp)

- 由公式可知，**语言模型越好，困惑度越小。**